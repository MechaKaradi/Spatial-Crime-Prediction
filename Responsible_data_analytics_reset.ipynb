{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31f3b48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modelling crime risk associated with Green Spaces\n",
    "---\n",
    "A case study of New York City using NYPD Complaint Data and Machine Learning models\n",
    "\n",
    "| Authors                        | Student ID   |\n",
    "|--------------------------------|--------------|\n",
    "| Kaninik Baradi                 | 5216664      |\n",
    "| Lala Sayyida Millati Nadhira   | 5844266      |\n",
    "| Rezzy Yolanda Wulandhari       | 4779487      |\n",
    "| Kelvin Engee                   | 4664043      |\n",
    "| Philippe Almeida Mirault       | 5898803      |\n",
    "Group 4\n",
    "\n",
    "For the course: Responsible Data Analytics, SEN 163B\n",
    "April 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f14471",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "This notebook is a companion to the report on Modelling Crime Risks associated with Green spaces. It contains all the code required to download, pre-process and analyse the data. The notebook is divided into the following sections:\n",
    "- Preparation\n",
    "- Descriptive Analytics\n",
    "    - Preliminary Analysis\n",
    "- Diagnostic Analytics\n",
    "    - Feature Analysis\n",
    "    - Feature Engineering\n",
    "    - Bias Analysis\n",
    "- Predictive Analysis\n",
    "    - Target Variable Analysis\n",
    "    - Train-Test Split Strategy\n",
    "    - Model Evaluation\n",
    "    - Model Interpretation\n",
    "- Prescriptive Analysis\n",
    "    - Cross Validation\n",
    "    - Ensemble Predictor\n",
    "    - Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0723a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9c8b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dependencies:\n",
    "\n",
    "Data Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7299df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install aequitas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_data = '..//data//' # path to the data folder\n",
    "\n",
    "testing_mode = False # set to True to run the notebook in testing mode with reduced data imports\n",
    "force_read_csv = False # set to True to force the notebook to read the csv files instead of the parquet files\n",
    "\n",
    "# write a function to use curl to download the data from the url\n",
    "def download_data(url, path):\n",
    "    # check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        # if the file does not exist, download it\n",
    "        !curl -o $path $url\n",
    "\n",
    "# download the data from the url and save it to the path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import Dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import altair\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise a seaborn theme for printing\n",
    "# Set the context to \"paper\"\n",
    "sns.set_context(\"paper\")\n",
    "# Set the style to \"ticks\"\n",
    "sns.set_style(\"ticks\")\n",
    "# Set the font scale\n",
    "sns.set(font_scale=1.5)\n",
    "# Set the palette to a colorblind-friendly palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Descriptive Analytics\n",
    "This section contains the initial analysis of the selected data sets. It identifies the underlying relationships of the variables and if used to determine variables of interest for further analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complaints Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in the data from the NYPD Complaint Data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the file complaint_data.parquet exists in the data folder or if the force_read_csv flag is set to True\n",
    "if force_read_csv or not os.path.exists(path_to_data + 'NYPD_Complaint_Data_Historic.parquet'):\n",
    "    # If the file does not exist, read the data from the csv file\n",
    "    # and save it as a parquet file\n",
    "    # Create a function that selects every 10th row\n",
    "    def every_nth(n):\n",
    "        return n % 10 == 0\n",
    "    if testing_mode:\n",
    "        skipping = lambda i: i>0 and every_nth(i)\n",
    "    else:\n",
    "        skipping = None\n",
    "\n",
    "    crime_data = pandas.read_csv(path_to_data + 'NYPD_Complaint_Data_Historic.csv', skiprows=skipping)\n",
    "\n",
    "    def make_categorical(dataframe, column):\n",
    "        # make the column of the dataframe categorical\n",
    "        dataframe[column] = dataframe[column].astype('category')\n",
    "\n",
    "    # scrub for duplicate\n",
    "    crime_data.drop_duplicates(inplace=True)\n",
    "    # get a lst of all the columns in the dataframe that need to be categorical\n",
    "    columns = ['ADDR_PCT_CD', 'BORO_NM', 'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE', 'JURIS_DESC', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PARKS_NM', 'PATROL_BORO', 'PD_CD', 'PD_DESC', 'STATION_NAME', 'SUSP_RACE', 'SUSP_SEX', 'TRANSIT_DISTRICT', 'VIC_RACE', 'VIC_SEX']\n",
    "\n",
    "    # make each column categorical\n",
    "    for column in columns:\n",
    "        make_categorical(crime_data, column)\n",
    "\n",
    "    crime_data = crime_data[['CMPLNT_FR_DT','CMPLNT_FR_TM','Longitude','Latitude','VIC_SEX','VIC_RACE','VIC_AGE_GROUP']]\n",
    "\n",
    "    # Save the data as a parquet file\n",
    "    crime_data.to_parquet(path_to_data + 'NYPD_Complaint_Data_Historic.parquet')\n",
    "\n",
    "else:\n",
    "    # If the file exists, read the data from the parquet file\n",
    "    crime_data = pandas.read_parquet(path_to_data + 'NYPD_Complaint_Data_Historic.parquet')\n",
    "    # if in testing mode sample 10% of the data\n",
    "    if testing_mode:\n",
    "        crime_data = crime_data.sample(frac=0.1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scrub for irrelevant data (only use required columns)\n",
    "crime_data = crime_data[['CMPLNT_FR_DT','CMPLNT_FR_TM','Longitude','Latitude','VIC_SEX','VIC_RACE','VIC_AGE_GROUP']]\n",
    "\n",
    "# All missing data from the selected columns is dropped as it will negatively impact the analysis, and the data is missing completely at random\n",
    "crime_data = crime_data.dropna(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transforming Complaint Date\n",
    "The date recorded for the estimated start time of the complaint `CMPLNT_FR_DT` is transformed to the year and the, day of the week. The day of the week is used to determine if the complaints are more likely to occur on a weekend or a weekday. The year is used to determine if the crime is more likely to occur in a certain year."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data['year'] = crime_data['CMPLNT_FR_DT'].str.split('/',expand=True)[2].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data = crime_data.loc[crime_data['year'] >= 2006]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data['CMPLNT_FR_DT'] = pandas.to_datetime(crime_data['CMPLNT_FR_DT'], format='%m/%d/%Y')\n",
    "crime_data['day_of_week'] = crime_data['CMPLNT_FR_DT'].dt.dayofweek"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data = crime_data.drop('CMPLNT_FR_DT', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot a bar plot of the number of complaints per year using seaborn\n",
    "sns.countplot(x='year', data=crime_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transforming Complaint time\n",
    "The time recorded for the estimated start time of the complaint `CMPLNT_FR_TM` is transformed to the hour. The hour is used to determine if the complaints are more likely to occur at a certain time of the day. The second and minute are dropped as they are not relevant to the analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data['hour'] = crime_data['CMPLNT_FR_TM'].str.split(':',expand=True)[0].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data = crime_data.drop('CMPLNT_FR_TM', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Removing non-human victims\n",
    "The data set contains a number of complaints where the victim is either not an individual that is a legal person, of the abstract concept of the people. These complaints are removed from the data set as they are not relevant to the analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sexes = ['M', 'F', 'U']\n",
    "\n",
    "crime_data = crime_data.loc[crime_data['VIC_SEX'].isin(sexes)]\n",
    "# Re calculate the categories for the Sex column\n",
    "crime_data['VIC_SEX'] = crime_data['VIC_SEX'].cat.remove_unused_categories()\n",
    "\n",
    "del sexes\n",
    "crime_data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove Unknown Race\n",
    "Race is used in the model to check model fairness and performance. Race is Unknown in some cases, and these cases are removed from the data set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep everything except for UNKNOWN\n",
    "races = ['WHITE', 'WHITE HISPANIC', 'BLACK','ASIAN / PACIFIC ISLANDER', 'BLACK HISPANIC','AMERICAN INDIAN/ALASKAN NATIVE', 'OTHER']\n",
    "\n",
    "crime_data = crime_data.loc[crime_data['VIC_RACE'].isin(races)]\n",
    "# Re calculate the categories for the Sex column\n",
    "crime_data['VIC_RACE'] = crime_data['VIC_RACE'].cat.remove_unused_categories()\n",
    "\n",
    "del races"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove Unknown or corrupted age\n",
    "Most age values are expressed as a range. In cases where the age value is not correctly recorded, the age is removed from the data set. This is done as the age is used to determine if the crime is more likely to occur to a certain age group."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.len()==4, None, crime_data['VIC_AGE_GROUP'])\n",
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.startswith(\"-\"), None, crime_data['VIC_AGE_GROUP'])\n",
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.contains(\"<\"), crime_data['VIC_AGE_GROUP'] + ' ', crime_data['VIC_AGE_GROUP'])\n",
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.endswith(\"+\"), crime_data['VIC_AGE_GROUP'] + ' ', crime_data['VIC_AGE_GROUP'])\n",
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.contains(\"UNKNOWN\"), None, crime_data['VIC_AGE_GROUP'])\n",
    "crime_data['VIC_AGE_GROUP'] = numpy.where(crime_data['VIC_AGE_GROUP'].str.len()==3, None, crime_data['VIC_AGE_GROUP'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#delete None\n",
    "crime_data = crime_data.dropna(subset=['VIC_AGE_GROUP'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data['VIC_AGE_GROUP'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### One hot encoding\n",
    "To indepedently evaluate the risks assocaited with sub-groups of the population, the categorical variables are one-hot encoded. Each group identifier is one-hot encoded, and these are later summed to evaluate the risk associated with each group."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_hot_encoded = pandas.get_dummies(crime_data[['VIC_SEX','VIC_RACE','VIC_AGE_GROUP']])\n",
    "one_hot_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# concatenate the one-hot encoded columns with the original dataframe\n",
    "crime_data = pandas.concat([crime_data, one_hot_encoded], axis=1)\n",
    "del one_hot_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop the VIC_SEX, VIC_Race, and VIC_AGE_GROUP columns\n",
    "crime_data = crime_data.drop('VIC_SEX', axis=1)\n",
    "crime_data = crime_data.drop('VIC_RACE', axis=1)\n",
    "crime_data = crime_data.drop('VIC_AGE_GROUP', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform the Latitude and Longitude\n",
    "The probability of a complaint originating at any individaul arbitrarily specific point will always be close to zero. To enable a meaningful prediction, a coarse grid of related points needs to be developed. The following code defines a predictive grid which can be used to predict the probability of a crime occurring within any block of the city."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First a base grid is defined using the minimum and maximum bounds of the NYPD jurisdiction. The grid is defined as a 100 x 100 grid, and the coordinates are rounded to 5 decimal places."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a grid of points across the precincts to use as the center of the crime clusters\n",
    "# The size of the grid is n x n, where n is the number of points in each direction\n",
    "from shapely.geometry import Point, Polygon\n",
    "from rtree import index\n",
    "\n",
    "precint_footprint = gpd.read_file('..//data//Police Precincts.geojson')\n",
    "\n",
    "# get the bounds of the precincts\n",
    "min_x, min_y, max_x, max_y = precint_footprint.total_bounds\n",
    "\n",
    "idx = index.Index()\n",
    "for i, row in precint_footprint.iterrows():\n",
    "    idx.insert(i, row.geometry.bounds)\n",
    "\n",
    "grid_size = 100  # You can adjust this value\n",
    "x_points = np.linspace(min_x, max_x, grid_size)\n",
    "y_points = np.linspace(min_y, max_y, grid_size)\n",
    "\n",
    "# Round the points to 4 decimal places\n",
    "x_points = np.around(x_points, 5)\n",
    "y_points = np.around(y_points, 5)\n",
    "\n",
    "grid = [Point(x, y) for x in x_points for y in y_points]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The grid is then filtered using the polygon of the NYPD jurisdiction. This is done to remove points that are outside the jurisdiction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# drop the points that are not within the precincts\n",
    "def is_point_inside_precincts(point, precincts_gdf, idx):\n",
    "    for i in idx.intersection(point.bounds):\n",
    "        if point.within(precincts_gdf.iloc[i].geometry):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "filtered_grid = [point for point in grid if is_point_inside_precincts(point, precint_footprint, idx)]\n",
    "\n",
    "# create a dataframe of the filtered grid\n",
    "filtered_grid_df = pandas.DataFrame([(point.x, point.y) for point in filtered_grid], columns=['Longitude', 'Latitude'])\n",
    "#\n",
    "del grid\n",
    "del filtered_grid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the polygon using geopandas.plot()\n",
    "precint_footprint.plot(ax=ax, facecolor=\"gray\", alpha=0.5)\n",
    "\n",
    "# Plot the points using seaborn.scatterplot()\n",
    "sns.scatterplot(x=\"Longitude\", y=\"Latitude\", data=filtered_grid_df, color=\"red\", s=5, ax=ax)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title(\"Polygon and Point Data\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "del fig, ax # remove the figure and axis object to free up memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All locations, of both crimes and trees will be transformed to a grid point. The following code uses a KD Tree to find the closest grid point to each crime. The grid point is then used as the location of the crime."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# create a KD Tree with the Longitude and Latitude columns of filtered_grid_df\n",
    "kd_tree = cKDTree(filtered_grid_df[['Longitude', 'Latitude']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# query the KD Tree with the Longitude and Latitude columns of crime_data\n",
    "distances, indices = kd_tree.query(crime_data[['Longitude', 'Latitude']])\n",
    "\n",
    "# use the indices to get the corresponding Longitude and Latitude values from filtered_grid_df\n",
    "crime_data['Longitude'] = filtered_grid_df.loc[indices, 'Longitude'].values\n",
    "crime_data['Latitude'] = filtered_grid_df.loc[indices, 'Latitude'].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the polygon using geopandas.plot()\n",
    "precint_footprint.plot(ax=ax, facecolor=\"gray\", alpha=0.5)\n",
    "\n",
    "# Plot the points using seaborn.scatterplot()\n",
    "sns.scatterplot(x=\"Longitude\", y=\"Latitude\", data=crime_data, color=\"red\", s=5, ax=ax)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title(\"Polygon and Point Data\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "del fig, ax # remove the figure and axis object to free up memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data = crime_data.loc[crime_data['Longitude'] > -74.5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data = crime_data.loc[crime_data['Latitude'] < 42.5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Group the Crime data\n",
    "Sum the number of complaints for each unique point, hour of the day, day of the week, and year, over the various attributes of the victim of the crime."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_counts = (\n",
    "    crime_data.groupby(['Longitude', 'Latitude', 'year', 'day_of_week', 'hour'])\n",
    "    .sum()\n",
    ")\n",
    "crime_counts.reset_index(inplace=True)\n",
    "del crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Street Trees Data\n",
    "Data from the 2015 street tree census is used to estimate the impact of hte number of trees on the number of crimes. The data is available from the [NYC Open Data Portal](). The 'health' attribute of the trees is used to evaluate if the level of up-keep of the trees has any predictive power."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if force_read_csv or not os.path.exists(path_to_data + '2015 Street Tree Census - Tree Data.parquet'):\n",
    "    # If the file does not exist, read the data from the csv file\n",
    "    # and save it as a parquet file\n",
    "    # Create a function that selects every 10th row\n",
    "    if testing_mode:\n",
    "        rows = 20000\n",
    "    else:\n",
    "        rows = None\n",
    "    trees = gpd.read_file(path_to_data + '2015 Street Tree Census - Tree Data.geojson', rows=rows)\n",
    "    trees = trees[['health', 'longitude', 'latitude']]\n",
    "\n",
    "    # Save the data as a parquet file\n",
    "    trees.to_parquet(path_to_data + '2015 Street Tree Census - Tree Data.parquet')\n",
    "\n",
    "else:\n",
    "    # If the file exists, read the data from the parquet file\n",
    "    trees = pandas.read_parquet(path_to_data + '2015 Street Tree Census - Tree Data.parquet')\n",
    "    if testing_mode:\n",
    "        trees = trees.sample(20000)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# query the KD Tree with the Longitude and Latitude columns of crime_data\n",
    "distances, indices = kd_tree.query(trees[['longitude', 'latitude']])\n",
    "\n",
    "# use the indices to get the corresponding Longitude and Latitude values from filtered_grid_df\n",
    "trees['Longitude'] = filtered_grid_df.loc[indices, 'Longitude'].values\n",
    "trees['Latitude'] = filtered_grid_df.loc[indices, 'Latitude'].values\n",
    "\n",
    "tree_counts = (\n",
    "    trees.groupby(['Longitude', 'Latitude','health'])\n",
    "    .size()\n",
    "    .reset_index(name='tree_count')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_counts.head()\n",
    "\n",
    "tree_counts_pivoted = pandas.pivot_table(tree_counts, values='tree_count', index=['Longitude', 'Latitude'], columns=['health'], fill_value=0)\n",
    "tree_counts_pivoted = tree_counts_pivoted.reset_index().rename(columns={'Good': 'good_tree_count', 'Fair': 'fair_tree_count', 'Poor': 'poor_tree_count'})\n",
    "\n",
    "# Remove index name\n",
    "tree_counts_pivoted.index.name = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Engineer zero Values\n",
    "Since the data set only contains reported complaints, it is necessary to infer and engineer examples of times and places from which complaints do not originate. To do this, a default matrix of zeros is created, and then the actual data is merged into the matrix.\n",
    "\n",
    "The matrix covers all points and all permutations of times at which the crime could have occurred. The times are the hours of the day, the days of the week, and the years in the data set. The points are the grid points that were created earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def point_hour_day_combinations(points, hours, days_of_week, years):\n",
    "    for point, hour, day, year in itertools.product(points, hours, days_of_week, years):\n",
    "        yield point.x, point.y, hour, day, year\n",
    "\n",
    "# Convert filtered grid points to a list of Point objects\n",
    "points = [Point(lon, lat) for lon, lat in filtered_grid_df[['Longitude', 'Latitude']].values]\n",
    "\n",
    "# Define hours and days_of_week\n",
    "hours = range(24)\n",
    "days_of_week = range(7)\n",
    "years = range(2006, 2022)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_combinations = np.array(list(point_hour_day_combinations(points, hours, days_of_week, years)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reduce points for testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if testing_mode:\n",
    "    #REDUCE POINTS FOR TESTING\n",
    "    # use numpy to drop all points that are left of the longitude of the mid point of the grid\n",
    "    all_combinations = all_combinations[all_combinations[:, 0] > filtered_grid_df['Longitude'].median()]\n",
    "    # use numpy to drop all points that are above the latitude of the mid point of the grid\n",
    "    all_combinations = all_combinations[all_combinations[:, 1] < filtered_grid_df['Latitude'].median()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_counts.set_index(['Longitude', 'Latitude', 'hour', 'day_of_week', 'year'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matched_data = crime_counts.reindex(\n",
    "    pandas.MultiIndex.from_arrays(all_combinations.T, names=crime_counts.index.names),\n",
    "    fill_value=0\n",
    ")\n",
    "matched_data.reset_index(inplace=True)\n",
    "\n",
    "matched_data = matched_data.merge(tree_counts_pivoted, on=['Longitude', 'Latitude'], how='left').fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matched_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define risk count as the sum of all columns that start with 'VIC'\n",
    "matched_data['crime_count'] = matched_data.filter(regex='VIC').sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matched_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create columns in matched_data called Risk_level\n",
    "# if the sum of the crime counts is 0 then risk level is 0, if it is greater than 0 and less than 4 then risk level is 1, if it is greater than 3 then risk level is 2 and if it is greater than 6 then risk level is 3\n",
    "def risk_level(crime_count):\n",
    "    if crime_count == 0:\n",
    "        return 0\n",
    "    elif 0 < crime_count < 4:\n",
    "        return 1\n",
    "    elif 3 < crime_count < 7:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "matched_data['Risk_level'] = matched_data['crime_count'].apply(risk_level)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# export matched_data to a parquet file\n",
    "matched_data.to_parquet('..\\\\data\\\\matched_data.parquet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LAB 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proxies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_matrix = matched_data.corr().round(2)\n",
    "# print(corr_matrix)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,40))       \n",
    "sns.heatmap(corr_matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag', mask=mask, ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representation bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_data2010 = matched_data.loc[matched_data['year'] == 2010]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_data2010"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_male = merged_data2010['VIC_SEX_M'].sum()\n",
    "total_female = merged_data2010['VIC_SEX_F'].sum()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = ['F', 'M']  # categories\n",
    "y = [total_female, total_male]  # values\n",
    "bin_edges = range(len(x) + 1)  # define bin edges\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('Sex of victims')\n",
    "plt.ylabel('Number of victims')\n",
    "plt.title('Sex distribution of victims')\n",
    "plt.plot(range(len(x)), [167849, 151816], 'o--', c='red', linewidth=0, markersize=8, label='Expected sex distribution according to New York Census 2010')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_18 = merged_data2010.iloc[:, 17].sum()\n",
    "total_24 = merged_data2010['VIC_AGE_GROUP_18-24'].sum()\n",
    "total_44 = merged_data2010['VIC_AGE_GROUP_25-44'].sum()\n",
    "total_64 = merged_data2010['VIC_AGE_GROUP_45-64'].sum()\n",
    "total_65 = merged_data2010.iloc[:, 16].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = ['<18', '18-24', '25-44', '45-64', '65+']\n",
    "y = [total_18, total_24, total_44, total_64, total_65]\n",
    "\n",
    "bin_edges = np.arange(len(x) + 1) - 0.5  # add a half bin width to shift the edges to the center of the bars\n",
    "bin_edges = range(len(x) + 1)  # define bin edges\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('Age group of victims')\n",
    "plt.ylabel('Number of victims')\n",
    "plt.title('Age group distribution of victims')\n",
    "plt.plot(range(len(x)), [69137, 33993, 99598, 78102, 38835], 'o--', c='red', linewidth=0, markersize=8, label='Expected age distribution according to New York Census 2010')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "white_hispanic = merged_data2010['VIC_RACE_WHITE HISPANIC'].sum()\n",
    "black = merged_data2010['VIC_RACE_BLACK'].sum()\n",
    "white = merged_data2010['VIC_RACE_WHITE'].sum()\n",
    "black_hispanic = merged_data2010['VIC_RACE_BLACK HISPANIC'].sum()\n",
    "asian_pacific = merged_data2010['VIC_RACE_ASIAN / PACIFIC ISLANDER'].sum()\n",
    "american_alaskan = merged_data2010['VIC_RACE_AMERICAN INDIAN/ALASKAN NATIVE'].sum()\n",
    "other = merged_data2010['VIC_RACE_OTHER'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = ['WH','B','W','BH','A/P','A/A', 'Other']\n",
    "y = [white_hispanic, black, white, black_hispanic, asian_pacific, american_alaskan, other]\n",
    "\n",
    "bin_edges = np.arange(len(x) + 1) - 0.5  # add a half bin width to shift the edges to the center of the bars\n",
    "bin_edges = range(len(x) + 1)  # define bin edges\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('Race group of victims')\n",
    "plt.ylabel('Number of victims')\n",
    "plt.title('Race group distribution of victims')\n",
    "plt.plot(range(len(x)), [45673 , 72781, 106471, 45673, 40311, 681 ,2262], 'o--', c='red', linewidth=0, markersize=8, label='Expected race distribution according to New York Census 2010')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Historical bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_victim_male = 3882544 - merged_data2010['VIC_SEX_M'].sum()\n",
    "non_victim_female = 4292589 - merged_data2010['VIC_SEX_F'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Define the data and log scale it\n",
    "victims = [total_male, total_female]\n",
    "non_victims = [non_victim_male, non_victim_female]\n",
    "\n",
    "labels = ['male', 'female']\n",
    "colors = ['orange', 'blue']\n",
    "\n",
    "# Create the stacked bar plot\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, victims, color=colors[0], label='victims')\n",
    "plt.bar(x, non_victims, bottom=victims, color=colors[1], label='non_victims')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Plot with Two Numbers')\n",
    "plt.legend()\n",
    "\n",
    "# Set y-axis to log scale and adjust limits\n",
    "plt.yscale('log')\n",
    "plt.ylim([1, 10**7])\n",
    "\n",
    "# Set y-axis tick labels to display original scale values\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_victim_male"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_victim_18 = 1768111 - merged_data2010.iloc[:, 17].sum()\n",
    "non_victim_24 = merged_data2010['VIC_AGE_GROUP_18-24'].sum()\n",
    "non_victim_44 = merged_data2010['VIC_AGE_GROUP_25-44'].sum()\n",
    "non_victim_64 = merged_data2010['VIC_AGE_GROUP_45-64'].sum()\n",
    "non_victim_65 = merged_data2010.iloc[:, 16].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictive Analytics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trees_crimes_people_df = matched_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialise models for testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "regressor_names = [\n",
    "    \"Lasso\",\n",
    "    \"Elastic Net\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"K-Nearest Neighbours\"\n",
    "]\n",
    "\n",
    "classifier_names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Neural Net\",\n",
    "    \"K-Nearest Neighbours\"\n",
    "]\n",
    "\n",
    "regressors = [\n",
    "    Lasso(alpha=0.1),\n",
    "    ElasticNet(random_state=0),\n",
    "    DecisionTreeRegressor(max_depth=5),\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPRegressor(alpha=1, max_iter=1000),\n",
    "    KNeighborsRegressor(3),\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=0),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    KNeighborsClassifier(3),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test different Classification models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Slide the dataframe and move colums 4 and 5 to df Y. Keep the rest in df X\n",
    "Y = trees_crimes_people_df.iloc[:, [24]]\n",
    "\n",
    "X = trees_crimes_people_df.iloc[:,[0,1,2,3,4,20,21,22]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "# Convert Y_train and Y_test to 1D array\n",
    "Y_train = Y_train.to_numpy().ravel()\n",
    "Y_test = Y_test.to_numpy().ravel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline: Naive Bayes Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Dropped categorical):  0.7713048958771799\n",
      "F1 (Dropped categorical):  0.7897075714691674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "Y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy (Dropped categorical): \", accuracy_score(Y_test, Y_pred))\n",
    "print(\"F1 (Dropped categorical): \", f1_score(Y_test, Y_pred, average='weighted'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Learning Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_technique_list = [\"Just-One\"]\n",
    "X_train_list = [X_train]\n",
    "X_test_list = [X_test]\n",
    "\n",
    "accuracy_per_dataset_df = pandas.DataFrame(columns=[\"Dataset Name\"].append(classifier_names))\n",
    "balanced_accuracy_per_dataset_df = pandas.DataFrame(columns=[\"Dataset Name\"].append(classifier_names))\n",
    "f1_score_per_dataset_df = pandas.DataFrame(columns=[\"Dataset Name\"].append(classifier_names))\n",
    "\n",
    "for technique,X_train,X_test in zip(categorical_technique_list,X_train_list,X_test_list):\n",
    "    print(\"[INFO] - Categorical technique: \", technique)\n",
    "    accuracy_line = {\"Dataset Name\": technique}\n",
    "    balanced_accuracy_line = {\"Dataset Name\": technique}\n",
    "    f1_score_line = {\"Dataset Name\": technique}\n",
    "\n",
    "    for classifier,method_name in zip(classifiers,classifier_names):\n",
    "        print(\"[INFO] - Classifier: \", method_name)\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        Y_pred = classifier.predict(X_test)\n",
    "        accuracy_line[method_name] = accuracy_score(Y_test,Y_pred)\n",
    "        balanced_accuracy_line[method_name] = balanced_accuracy_score(Y_test,Y_pred)\n",
    "        f1_score_line[method_name] = f1_score(Y_test,Y_pred, average='weighted')\n",
    "\n",
    "    accuracy_per_dataset_df = pandas.concat([accuracy_per_dataset_df, pandas.DataFrame(accuracy_line.values(), index=accuracy_line.keys()).T], ignore_index=True)\n",
    "    balanced_accuracy_per_dataset_df = pandas.concat([balanced_accuracy_per_dataset_df, pandas.DataFrame(balanced_accuracy_line.values(), index=balanced_accuracy_line.keys()).T], ignore_index=True)\n",
    "    f1_score_per_dataset_df = pandas.concat([f1_score_per_dataset_df, pandas.DataFrame(f1_score_line.values(), index=f1_score_line.keys()).T], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unpivot the dataframe\n",
    "accuracy_per_dataset_df = accuracy_per_dataset_df.melt(id_vars=['Dataset Name'], var_name='Method', value_name='Accuracy')\n",
    "accuracy_per_dataset_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unpivot the dataframe\n",
    "balanced_accuracy_per_dataset_df = balanced_accuracy_per_dataset_df.melt(id_vars=['Dataset Name'], var_name='Method', value_name='Balanced Accuracy')\n",
    "balanced_accuracy_per_dataset_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unpivot the dataframe\n",
    "f1_score_per_dataset_df = f1_score_per_dataset_df.melt(id_vars=['Dataset Name'], var_name='Method', value_name='F1 Score')\n",
    "f1_score_per_dataset_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lab 6\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KNeighborsClassifier(3).fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction['real_risk'] = Y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction['predicted_risk'] = Y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction['predicted_risk'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction['VIC_AGE_GROUP'] == df[['dummy1', 'dummy2']].apply(lambda x: x.idxmax(), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction.iloc[:,5].min()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pandas.DataFrame({'dummy1': [1, 0, 1], 'dummy2': [0, 1, 1], 'dummy3': [1, 1, 0]})\n",
    "\n",
    "# use apply to find the column with the highest value (i.e., 1) in each row, for dummy1 and dummy2 only\n",
    "df['combined'] = df[['dummy1', 'dummy2']].apply(lambda x: x.idxmax(), axis=1)\n",
    "\n",
    "# display the updated DataFrame\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group = Group()\n",
    "xtab, _ = group.get_crosstabs(prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating Model Performance - Future Predictions\n",
    "To evaluate the predictive performance of the basline model in the real world, the performance of the model is evaluated for a year that the model does not have access to . Data from the year 2021 is held out from the model and used for the model testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 8)) while a minimum of 1 is required by KNeighborsClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[113], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m knn\u001B[38;5;241m.\u001B[39mfit(X_train, Y_train)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Predict the Y test data\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m Y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mknn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Calculate the accuracy of the model\u001B[39;00m\n\u001B[0;32m     25\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(Y_test,Y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001B[0m, in \u001B[0;36mKNeighborsClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m    Class labels for each data sample.\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;66;03m# In that case, we do not need the distances to perform\u001B[39;00m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;66;03m# the weighting so we do not compute them.\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m     neigh_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m     neigh_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\neighbors\\_base.py:806\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    804\u001B[0m         X \u001B[38;5;241m=\u001B[39m _check_precomputed(X)\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 806\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    808\u001B[0m n_samples_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_fit_\n\u001B[0;32m    809\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_neighbors \u001B[38;5;241m>\u001B[39m n_samples_fit:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\base.py:565\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 565\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    929\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m--> 931\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    932\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    933\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    934\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m    935\u001B[0m         )\n\u001B[0;32m    937\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    938\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 8)) while a minimum of 1 is required by KNeighborsClassifier."
     ]
    }
   ],
   "source": [
    "# Divide the trees_crimes_people_df into training and testing sets based on the year\n",
    "trees_crimes_people_df_2021 = trees_crimes_people_df[trees_crimes_people_df['year'] == 2021]\n",
    "trees_crimes_people_df_rest = trees_crimes_people_df[trees_crimes_people_df['year'] != 2021]\n",
    "\n",
    "# Define the X train and Y train data on the trees_crimes_people_df_rest\n",
    "X_train = trees_crimes_people_df_rest.iloc[:,[0,1,2,3,4,20,21,22]]\n",
    "Y_train = trees_crimes_people_df_rest.iloc[:, [24]]\n",
    "\n",
    "# Define the X test and Y test data on the trees_crimes_people_df_2021\n",
    "X_test = trees_crimes_people_df_2021.iloc[:,[0,1,2,3,4,20,21,22]]\n",
    "Y_test = trees_crimes_people_df_2021.iloc[:, [24]]\n",
    "\n",
    "# Convert Y_train and Y_test to 1D array\n",
    "Y_train = Y_train.to_numpy().ravel()\n",
    "Y_test = Y_test.to_numpy().ravel()\n",
    "\n",
    "# Run the k-NN model on the X train and Y train data\n",
    "knn = KNeighborsClassifier(5)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the Y test data\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(Y_test,Y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "balanced_accuracy = balanced_accuracy_score(Y_test,Y_pred)\n",
    "print(\"Balanced Accuracy: \", balanced_accuracy)\n",
    "f1_score = f1_score(Y_test,Y_pred, average='weighted')\n",
    "print(\"F1 Score: \", f1_score)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating Model Performance - K folds validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "# the number of folds is 5\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Define the X and Y data\n",
    "X = trees_crimes_people_df.iloc[:,[0,1,2,3,4,20,21,22]]\n",
    "Y = trees_crimes_people_df.iloc[:, [24]]\n",
    "\n",
    "# Convert Y to 1D array\n",
    "Y = Y.to_numpy().ravel()\n",
    "\n",
    "# Define the accuracy, balanced accuracy and f1 score lists\n",
    "accuracy_list = []\n",
    "balanced_accuracy_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# Iterate through the folds\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print the fold number being processed\n",
    "    print(\"Fold: \", len(accuracy_list)+1)\n",
    "    # Split the X and Y data into train and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Run the k-NN model on the X train and Y train data\n",
    "    knn = KNeighborsClassifier(5)\n",
    "    # Print the training\n",
    "    print(\"Training...\")\n",
    "    knn.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict the Y test data\n",
    "    print(\"Predicting...\")\n",
    "    Y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(Y_test,Y_pred)\n",
    "    accuracy_list.append(accuracy)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test,Y_pred)\n",
    "    balanced_accuracy_list.append(balanced_accuracy)\n",
    "    f1_accuracy = f1_score(Y_test,Y_pred, average='weighted')\n",
    "    f1_score_list.append(f1_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the accuracy, balanced accuracy and f1 score lists\n",
    "print(\"Accuracy: \", accuracy_list)\n",
    "print(\"Balanced Accuracy: \", balanced_accuracy_list)\n",
    "print(\"F1 Score: \", f1_score_list)\n",
    "\n",
    "# plot the accuracy, balanced accuracy and f1 score lists with the number of folds and the mean of the scores\n",
    "plt.plot([1,2,3,4,5], accuracy_list, label = \"Accuracy\")\n",
    "plt.plot([1,2,3,4,5], balanced_accuracy_list, label = \"Balanced Accuracy\")\n",
    "plt.plot([1,2,3,4,5], f1_score_list, label = \"F1 Score\")\n",
    "plt.plot([1,2,3,4,5], [np.mean(accuracy_list)]*5, label = \"Mean Accuracy\")\n",
    "plt.plot([1,2,3,4,5], [np.mean(balanced_accuracy_list)]*5, label = \"Mean Balanced Accuracy\")\n",
    "plt.plot([1,2,3,4,5], [np.mean(f1_score_list)]*5, label = \"Mean F1 Score\")\n",
    "plt.xlabel(\"Number of Folds\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test different Regression models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y = trees_crimes_people_df.iloc[:, [23]]\n",
    "\n",
    "X = trees_crimes_people_df.iloc[:,[0,1,2,3,4,20,21,22]]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "# Convert Y_train and Y_test to 1D array\n",
    "Y_train = Y_train.to_numpy().ravel()\n",
    "Y_test = Y_test.to_numpy().ravel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline: Linear Regression Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(Y_test, Y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(Y_test, Y_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(Y_test, Y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the linear regression model using matplotlib\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99b1ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_technique_list = [\"Just one\"]\n",
    "X_train_list = [X_train]\n",
    "X_test_list = [X_test]\n",
    "\n",
    "# Create a dictionary to store the dataframes of the results for each method\n",
    "results_dict = {}\n",
    "\n",
    "MSE_per_dataset_df = pd.DataFrame(columns=[\"Dataset Name\"].append(names))\n",
    "MAE_per_dataset_df = pd.DataFrame(columns=[\"Dataset Name\"].append(names))\n",
    "MAPE_per_dataset_df = pd.DataFrame(columns=[\"Dataset Name\"].append(names))\n",
    "\n",
    "for technique,X_train,X_test in zip(categorical_technique_list,X_train_list,X_test_list):\n",
    "    print(\"[INFO] - Categorical technique: \", technique)\n",
    "    MSE_line = {\"Dataset Name\": technique}\n",
    "    MAE_line = {\"Dataset Name\": technique}\n",
    "    MAPE_line = {\"Dataset Name\": technique}\n",
    "\n",
    "    for regressor,method_name in zip(regressors,regressor_names):\n",
    "        print(\"[INFO] - Regressor: \", method_name)\n",
    "        regressor.fit(X_train, Y_train)\n",
    "        Y_pred = regressor.predict(X_test)\n",
    "        # create a dataframe with the Y_test and Y_pred\n",
    "        Y_test_Y_pred_df = pd.DataFrame({'Y_test': Y_test, 'Y_pred': Y_pred})\n",
    "        # append the dataframe to the dictionary\n",
    "        results_dict[method_name] = Y_test_Y_pred_df\n",
    "        # calculate the MSE, MAE and MAPE\n",
    "        MSE_line[method_name] = mean_squared_error(Y_test, Y_pred)\n",
    "        MAE_line[method_name] = mean_absolute_error(Y_test, Y_pred)\n",
    "        MAPE_line[method_name] = mean_absolute_percentage_error(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "    # using pandas concat to append the new line to the dataframe\n",
    "    MSE_per_dataset_df = pandas.concat([MSE_per_dataset_df, pd.DataFrame([MSE_line])], ignore_index=True)\n",
    "    MAE_per_dataset_df = pandas.concat([MAE_per_dataset_df, pd.DataFrame([MAE_line])], ignore_index=True)\n",
    "    MAPE_per_dataset_df = pandas.concat([MAPE_per_dataset_df, pd.DataFrame([MAPE_line])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef4772",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unpivot the dataframe\n",
    "MSE_per_dataset_df = MSE_per_dataset_df.melt(id_vars=['Dataset Name'], var_name='Method', value_name='MSE')\n",
    "MSE_per_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951764b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAE_per_dataset_df = MAE_per_dataset_df.melt(id_vars=['Dataset Name'], var_name='Method', value_name='MAE')\n",
    "MAE_per_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7f620",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAPE_per_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893908e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Intrinsic Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21735cb2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# define the model\n",
    "model = KNeighborsRegressor(3)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X_test, Y_test, scoring='neg_mean_squared_error')\n",
    "\n",
    "# get importance\n",
    "importance = results.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
